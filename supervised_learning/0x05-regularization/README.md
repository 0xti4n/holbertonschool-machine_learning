# Regularization

## Specializations - Machine Learning â€• Supervised Learning

## Description

* This repository contains some Regularization exercises

## Learning Objectives

**Understand:**

* What is regularization? What is its purpose?
* What is are L1 and L2 regularization?
* What is the difference between the two methods?
* What is dropout?
* What is early stopping?
* What is data augmentation?
* How do you implement the above regularization methods in Numpy? Tensorflow?
* What are the pros and cons of the above regularization methods?


## Dependencies
```
Python 3.5
numpy 1.15
tensorflow 1.12
```

## Repo content

* **Main Folder that contains all main of the following tasks:**

| Task | Description |
| --- | --- |
|**0. L2 Regularization Cost** | calculates the cost of a neural network with L2 regularization
|**1. Gradient Descent with L2 Regularization** | updates the weights and biases of a neural network using gradient descent with L2 regularization
|**2. L2 Regularization Cost** | calculates the cost of a neural network with L2 regularization
|**3. Create a Layer with L2 Regularization** | creates a tensorflow layer that includes L2 regularization
|**4. Forward Propagation with Dropout** | conducts forward propagation using Dropout
|**5. Gradient Descent with Dropout** | updates the weights of a neural network with Dropout regularization using gradient descent
|**6. Create a Layer with Dropout** | creates a layer of a neural network using dropout
|**7. Early Stopping** | determines if you should stop gradient descent early

## Usage
* Clone the repo and execute the main files

## Author
- [Cristian G](https://github.com/cristian-fg)

## License
[MIT](https://choosealicense.com/licenses/mit/)
